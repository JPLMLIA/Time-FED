{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_INPUT = '../../data/v2/data.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf(F_INPUT, 'merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df.index.dayofyear\n",
    "df['logCn2'] = np.log10(df['Cn2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feats = ['pressure', 'relative_humidity', 'temperature', 'wind_speed', 'logCn2', 'solar_zenith_angle','day']\n",
    "label_day = 'r0_day'\n",
    "label = 'r0'\n",
    "label_night = 'r0_night'\n",
    "feats_plus_r0 = feats + ['r0']\n",
    "feats_plus_r0day = feats + ['r0_day']\n",
    "feats_plus_r0night = feats + ['r0_night']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### restricting data to usable, relatively dense subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_subset = df[(df.index > '2018-05-03') & (df.index < '2020-12-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### finding non-nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ~df_subset[feats_plus_r0].isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[valid, feats_plus_r0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "valid_day = ~df_subset[feats_plus_r0day].isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[valid_day, feats_plus_r0day].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_night = ~df_subset[feats_plus_r0night].isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[valid_night,feats_plus_r0night].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_date = '2019-12-31'\n",
    "train = df_subset.index <= split_date\n",
    "test  = df_subset.index > split_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth_night = df_subset.loc[test&valid_night,label_night]\n",
    "test_truth_day = df_subset.loc[test&valid_day,label_day]\n",
    "test_truth_all = df_subset.loc[test&valid,label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[train&valid,feats_plus_r0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[test&valid,feats_plus_r0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[train&valid_day,feats_plus_r0day].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[test&valid_day,feats_plus_r0day].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[train&valid_night,feats_plus_r0night].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.loc[test&valid_night,feats_plus_r0night].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing the RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train and test subroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_test(train_df, test_df, feats, label):\n",
    "    regr.fit(train_df[feats], train_df[label])\n",
    "    r2 = regr.score(test_df[feats], test_df[label])\n",
    "    preds = regr.predict(test_df[feats])\n",
    "    return preds, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_preds_all, test_r2_all = train_and_test(df_subset.loc[train & valid], df_subset.loc[test & valid], feats, label)\n",
    "test_r2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error_diff(targ, pred):\n",
    "    return targ-pred\n",
    "def error_perc(targ, pred):\n",
    "    return (targ-pred)/targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(m1, m2, xmin, xmax, ymin, ymax):\n",
    "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]                                                     \n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])                                                       \n",
    "    values = np.vstack([m1, m2])                                                                        \n",
    "    kernel = stats.gaussian_kde(values)                                                                 \n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "    return X, Y, Z\n",
    "\n",
    "def scatter_with_errors(truth, preds, error_func, xmin, xmax, ymin, ymax):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    s = 25\n",
    "    a = 0.4\n",
    "    ax[0,0].scatter(truth, preds, edgecolor='k', c=\"cornflowerblue\", s=s, alpha=a)\n",
    "    x = np.linspace(truth.min(), truth.max(), 1000)\n",
    "    ax[0,0].plot(x, x, 'r-')\n",
    "    ax[0,0].set_xlabel(\"Actual r0\")\n",
    "    ax[0,0].set_ylabel(\"Predicted r0\")\n",
    "    ax[0,0].set_xlim([xmin, xmax])                                                                           \n",
    "    ax[0,0].set_ylim([ymin, ymax]) \n",
    "\n",
    "    \n",
    "    X, Y, Z = density_estimation(truth, preds, xmin, xmax, ymin, ymax)\n",
    "    ax[0,1].plot(x, x, 'r-')\n",
    "    im01 = ax[0,1].contourf(X, Y, Z, 20)\n",
    "    divider01 = make_axes_locatable(ax[0,1])\n",
    "    cax01 = divider01.append_axes('right', size='5%', pad=0.05)\n",
    "    ax[0,1].set_xlim([xmin, xmax])                                                                           \n",
    "    ax[0,1].set_ylim([ymin, ymax]) \n",
    "    fig.colorbar(im01, cax=cax01)\n",
    "\n",
    "\n",
    "    # now plot with errors\n",
    "    #\n",
    "    errs = error_func(truth, preds)\n",
    "    ymin = np.min(errs) - 1\n",
    "    ymax = np.max(errs) + 1\n",
    "    \n",
    "    \n",
    "    ax[1,0].scatter(truth, errs, edgecolor='k', c=\"forestgreen\", s=s, alpha=a)\n",
    "    ax[1,0].plot(x, np.zeros(x.shape), 'r-')\n",
    "    ax[1,0].set_xlabel(\"Actual r0\")\n",
    "    ax[1,0].set_ylabel(\"Error r0\")\n",
    "    ax[1,0].set_xlim([xmin, xmax])     \n",
    "    ax[1,0].set_ylim([ymin, ymax])                                                                           \n",
    "\n",
    "    \n",
    "    Xerr, Yerr, Zerr = density_estimation(truth, errs, xmin, xmax, ymin, ymax)\n",
    "    ax[1,1].plot(x, np.zeros(x.shape), 'r-')\n",
    "    im11 = ax[1,1].contourf(Xerr, Yerr, Zerr, 20)\n",
    "    divider11 = make_axes_locatable(ax[1,1])\n",
    "    cax11 = divider11.append_axes('right', size='5%', pad=0.05)\n",
    "    ax[1,1].set_xlim([xmin, xmax]) \n",
    "    ax[1,1].set_ylim([ymin, ymax])                                                                           \n",
    "    fig.colorbar(im11, cax=cax11)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### scatter plots of actual vs. predict using error_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xmin, ymin = 0, 0\n",
    "xmax, ymax = 20, 20\n",
    "scatter_with_errors(test_truth_all, test_preds_all, error_perc, xmin, xmax, ymin, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interactive time domain plot of errors\n",
    "\n",
    "switching matplotlib to notebook mode to enable a zoom-in of different portions of the time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_in_time(truth, preds):\n",
    "    fig, ax = plt.subplots(3, 1)\n",
    "    ax[0].plot(truth.index, truth, 'gx', label='actual')\n",
    "    ax[0].plot(truth.index, preds, 'ro', label='predicted')\n",
    "    ax[0].set_xlabel(\"Datetime\")\n",
    "    ax[0].set_ylabel(\"r0\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(truth.index, error_diff(truth, preds), 'bx')\n",
    "    ax[1].set_xlabel(\"Datetime\")\n",
    "    ax[1].set_ylabel(\"error r0\")\n",
    "\n",
    "    ax[2].plot(truth.index, error_perc(truth, preds), 'bx')\n",
    "    ax[2].set_xlabel(\"Datetime\")\n",
    "    ax[2].set_ylabel(\"perc error r0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "plot_errors_in_time(test_truth_all, test_preds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance\n",
    "\n",
    "feature importance from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_importance(forest, X, featnames):\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(f\"{f + 1}. {featnames[indices[f]]:20} ({importances[indices[f]]})\")\n",
    "\n",
    "    # Plot the impurity-based feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "            color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), [ featnames[i] for i in indices ], rotation='vertical')\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(regr, df_subset.loc[train&valid,feats], feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug why CN2 is so low\n",
    "\n",
    "#### What happens if I drop month and SZA\n",
    "\n",
    "Answer: turns out we had to take the log of CN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_no_sza = ['pressure', 'relative_humidity', 'temperature', 'wind_speed', 'logCn2']\n",
    "preds_all_no_sza, r2_all_no_sza = train_and_test(df_subset.loc[train & valid], df_subset.loc[test & valid], feats_no_sza, label)\n",
    "scatter_with_errors(test_truth_all, preds_all_no_sza, error_perc, xmin, xmax, ymin, ymax)\n",
    "plot_importance(regr, df_subset.loc[train&valid,feats_no_sza], feats_no_sza)\n",
    "r2_all_no_sza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation between the signals using [stats.pearsonr](https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9)\n",
    "\n",
    "We calculate:\n",
    "- overall synchrony between r0 and Cn2\n",
    "- local synchrony between r0 and Cn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Synchrony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_synchrony(feat1, feat2, feat1name, feat2name, r):\n",
    "    f,ax=plt.subplots(2, 1, figsize=(7,3), sharex=True)\n",
    "    ax[0].plot(feat1, label=feat1name)\n",
    "    ax[1].plot(feat2, label=feat2name)\n",
    "    ax[1].set(title=f\"Overall Pearson r = {np.round(r,2)}\");\n",
    "    return\n",
    "\n",
    "def plot_local_synchrony(feat1, feat2, feat1name, feat2name, r_window_size=120):\n",
    "    # Compute rolling window synchrony\n",
    "    rolling_r = feat1.rolling(window=r_window_size, center=True).corr(feat2)\n",
    "    f,ax=plt.subplots(3,1,figsize=(14,6),sharex=True)\n",
    "    ax[0].plot(feat1, label=feat1name)\n",
    "    ax[1].plot(feat2, label=feat2name)\n",
    "    rolling_r.plot(ax=ax[2])\n",
    "    ax[0].set(ylabel=feat1name)\n",
    "    ax[1].set(ylabel=feat2name)\n",
    "    ax[2].set(ylabel='Pearson r')\n",
    "    plt.suptitle(\"data and rolling window correlation\")\n",
    "    \n",
    "    \n",
    "def print_pearsonr(feat1, feat2):\n",
    "\n",
    "    r, p = stats.pearsonr(feat1, feat2)\n",
    "    print(f\"Scipy computed Pearson r: {r} and p-value: {p}\")\n",
    "    return r, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = print_pearsonr(df_subset.loc[train&valid,label], df_subset.loc[train&valid,'logCn2'])\n",
    "# plot_overall_synchrony(df_subset.loc[train&valid,label], df_subset.loc[train&valid,['logCn2']], label, 'logCn2', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Synchrony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_local_synchrony(df_subset.loc[train&valid,label], df_subset.loc[train&valid,['logCn2']], label, 'Cn2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchrony using only R0 daytime data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Synchrony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = print_pearsonr(df_subset.loc[train&valid_day,label_day], df_subset.loc[train&valid_day,'logCn2'])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_overall_synchrony(df_subset.loc[train&valid_day,label_day], df_subset.loc[train&valid_day,['logCn2']], label_day, 'logCn2', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Synchrony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_local_synchrony(df_subset.loc[train&valid_day,label_day], df_subset.loc[train&valid_day,['logCn2']], label_day, 'logCn2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Using R0 daytime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_day, test_r2_day = train_and_test(df_subset.loc[train & valid_day], df_subset.loc[test & valid_day], feats, label_day)\n",
    "scatter_with_errors(test_truth_day, test_preds_day, error_perc, xmin, xmax, ymin, ymax)\n",
    "plot_importance(regr, df_subset.loc[train&valid_day,feats], feats)\n",
    "test_r2_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier with Night-time r0 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_night, test_r2_night = train_and_test(df_subset.loc[train & valid_night], df_subset.loc[test & valid_night], feats, label_night)\n",
    "scatter_with_errors(test_truth_night, test_preds_night, error_perc, xmin, xmax, ymin, ymax)\n",
    "plot_importance(regr, df_subset.loc[train&valid_night,feats], feats)\n",
    "test_r2_night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms by Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r0_histograms(r0, r0min, r0max, key):\n",
    "    bins = np.linspace(r0min, r0max, 100)\n",
    "    plt.yscale('log')\n",
    "    plt.hist(r0, bins, histtype='step', label=key)\n",
    "    \n",
    "r0_histograms(test_truth_day, 0, 80, 'r0 day')\n",
    "r0_histograms(test_truth_night, 0, 80, 'r0 night')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Histograms by Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_by_r0_histograms(truth, errs, r0min, r0max):\n",
    "    plt.figure()\n",
    "    bins = np.linspace(r0min, r0max, 81)\n",
    "    d = {'r0_floor': np.floor(truth), 'errs': errs}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    errs_by_r0_mean = df.groupby('r0_floor').mean()\n",
    "    errs_by_r0_std = df.groupby('r0_floor').std()\n",
    "    x = sorted(df['r0_floor'].unique())\n",
    "    print(np.arange(0, max(x), 5))\n",
    "    plt.bar(x, errs_by_r0_mean['errs'], yerr=errs_by_r0_std['errs'])\n",
    "    plt.xticks(np.arange(0, max(x), 4))\n",
    "\n",
    "    plt.xlabel('r0 floor')\n",
    "    plt.ylabel('perc error')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "error_by_r0_histograms(test_truth_all, error_perc(test_truth_all, test_preds_all), 0, 80)\n",
    "error_by_r0_histograms(test_truth_day, error_perc(test_truth_day, test_preds_day), 0, 80)\n",
    "error_by_r0_histograms(test_truth_night, error_perc(test_truth_night, test_preds_night), 0, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
